{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semisupervised learning on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge for this project was to achieve good performance in a semi-supervised learning task on the MNIST dataset. Of the 50,000 training examples, labels are available for some small subset of these, between 100 and 500. The remainder are available as unlabeled images. Below, I discuss how various approaches fared, and how performance varied with different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice of supervised learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On review of the literature, some supervised learning approaches that featured prominently were:\n",
    "* Generative models:  (1) (2)\n",
    "* Semi-supervised support vector machines (1)\n",
    "* Self-training (2)\n",
    "* Cotraining and multiview models (1) (2)\n",
    "* Graph-based models (1) (2)\n",
    "\n",
    "In order to select one of these, my main criteria were that I wanted an algorithm with an efficient runtime that would achieve good performance with few labelled examples but very many unlablled ones. Since self-training, co-training and multiview models would usually running classifiers over the data many times, I ruled them out. Since the label propagation approach of [Zhu and Ghahramani](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.3864&rep=rep1&type=pdf) (3) has already demonstrated good performance on a digits dataset, I selected to apply it to MNIST.\n",
    "\n",
    "The algorithm used was an adapted version of the algorithm used by Zhu and Ghahramani. Since the MNIST dataset is fivefold larger than the dataset used in the original paper, it was necessary to adapt the algorithm so that it does not utilize squared distances between all datapoints. Instead, a sparse kernel was used that just stores the nearest neighbours of each datapoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main algorithm that was tried was label propagation. You can look over the source of the LabelPropagation class in ../models/label_propagation.py but the results are demonstrated below using five-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../dat/train-images-idx3-ubyte.gz\n",
      "Extracting ../dat/train-labels-idx1-ubyte.gz\n",
      "Extracting ../dat/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../dat/t10k-labels-idx1-ubyte.gz\n",
      "fold: 0 filling neighbour row 1 / 3100\n",
      "500 1000 1500 2000 2500 3000 fold: 0 filling neighbour row 1 / 3300\n",
      "500 1000 1500 2000 2500 3000 fold: 0 filling neighbour row 1 / 3500\n",
      "500 1000 1500 2000 2500 3000 fold: 1 filling neighbour row 1 / 3100\n",
      "500 1000 1500 2000 2500 3000 fold: 1 filling neighbour row 1 / 3300\n",
      "500 1000 1500 2000 2500 3000 fold: 1 filling neighbour row 1 / 3500\n",
      "500 1000 1500 2000 2500 3000 fold: 2 filling neighbour row 1 / 3100\n",
      "500 1000 1500 2000 2500 3000 fold: 2 filling neighbour row 1 / 3300\n",
      "500 1000 1500 2000 2500 3000 fold: 2 filling neighbour row 1 / 3500\n",
      "500 1000 1500 2000 2500 3000 fold: 3 filling neighbour row 1 / 3100\n",
      "500 1000 1500 2000 2500 3000 fold: 3 filling neighbour row 1 / 3300\n",
      "500 1000 1500 2000 2500 3000 fold: 3 filling neighbour row 1 / 3500\n",
      "500 1000 1500 2000 2500 3000 fold: 4 filling neighbour row 1 / 3100\n",
      "500 1000 1500 2000 2500 3000 fold: 4 filling neighbour row 1 / 3300\n",
      "500 1000 1500 2000 2500 3000 fold: 4 filling neighbour row 1 / 3500\n",
      "500 1000 1500 2000 2500 3000 "
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "%run '../models/label_propagation.py'\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "\n",
    "# get model\n",
    "mnist_dir = '../dat'          #the directory of mnist.ubyte files\n",
    "dat = read_data_sets(mnist_dir)\n",
    "train_sizes = [100, 300, 500]\n",
    "k_folds = 5\n",
    "\n",
    "sup_scores = np.zeros((len(train_sizes), k_folds)) # scores from using supervised knn only\n",
    "ssl_scores = np.zeros((len(train_sizes), k_folds)) # scores from using knn on propagated labels\n",
    "\n",
    "for fold in range(k_folds):\n",
    "    for j, labelled_examples in enumerate(train_sizes):\n",
    "        print('fold: {}'.format(fold), end=' ')\n",
    "        random_perm = np.random.permutation(range(dat.train.labels.shape[0]))\n",
    "        i_tr = random_perm[:labelled_examples]\n",
    "        i_unlabelled = random_perm[-3000:]\n",
    "        \n",
    "        Xtr = dat.train.images[i_tr]\n",
    "        ytr = dat.train.labels[i_tr]\n",
    "        ytr_onehot = np.eye(10)[ytr]\n",
    "        Xunl = dat.train.images[i_unlabelled]\n",
    "        Xval = dat.validation.images\n",
    "        yval = dat.validation.labels\n",
    "\n",
    "        lp = LabelPropagation(n_nearest_neighbours=10, iters=20, verbose = True) #detail of this class is in ../models/label_propgation.py\n",
    "        pred = lp.propagate(Xtr, ytr_onehot, Xunl)\n",
    "        \n",
    "        knn = KNeighborsClassifier()\n",
    "\n",
    "        knn.fit(np.vstack((Xtr,Xunl)), np.hstack((ytr, pred)))\n",
    "        ssl_scores[j, fold] = knn.score(Xval, yval)\n",
    "\n",
    "        knn.fit(Xtr, ytr)\n",
    "        sup_scores[j, fold] = knn.score(Xval, yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEhCAYAAACUW2yNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nteIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdf19598450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run '../models/label_propagation.py'\n",
    "plot_supervised_learner_performance(sup_scores, ssl_scores, title = 'Classification performance', xlab = 'Labelled examples', \n",
    "                                    ylab = 'Accuracy (zero/one loss)', ticklabs = [100,300,500], legend =['Sup', 'SSL'], ylim=(.5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So label propagation provides a clear and significant improvement to test set classification accuracy using a K nearest neighbour classifier. But how good can our classification accuracy get using these propagated labels? To get impressive performance, we need to find the nearest neighbours across a larger portion of the dataset, which we now unpack. (This takes about an hour on my machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filling neighbour row 1 / 55000\n",
      "500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500 6000 6500 7000 7500 8000 8500 9000 9500 10000 10500 11000 11500 12000 12500 13000 13500 14000 14500 15000 15500 16000 16500 17000 17500 18000 18500 19000 19500 20000 20500 21000 21500 22000 22500 23000 23500 24000 24500 25000 25500 26000 26500 27000 27500 28000 28500 29000 29500 30000 30500 31000 31500 32000 32500 33000 33500 34000 34500 35000 35500 36000 36500 37000 37500 "
     ]
    }
   ],
   "source": [
    "labelled_examples\n",
    "random_perm = np.random.permutation(range(dat.train.labels.shape[0]))\n",
    "i_tr = random_perm[:labelled_examples]\n",
    "i_unlab = random_perm[labelled_examples:]\n",
    "Xtr = dat.train.images[i_tr]\n",
    "ytr = np.eye(10)[dat.train.labels[i_tr]]\n",
    "Xunl = dat.train.images[i_unlab]\n",
    "\n",
    "lp = LabelPropagation(n_nearest_neighbours=10, iters=20, verbose = True)\n",
    "yprop = lp.propagate(Xtr, ytr, Xunl)\n",
    "yprop = np.eye(10)[yprop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train =np.vstack((ytr yprop))\n",
    "x_train = np.vstack((Xtr, Xunl))\n",
    "x_train = x_train.reshape(x_train.shape[0],1,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a standard modern convolutional neural network with 4 layers, dropout, relu activations and light weight regularization, and train this on labels that were propagated using the previous technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.regularizers import WeightRegularizer\n",
    "\n",
    "\n",
    "def create_model(img_rows, img_cols,learning_rate, reg, decay, momentum=.9, dropout = .5):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(128, 5, 5, border_mode='same', init='he_normal', W_regularizer=WeightRegularizer(l1=reg),\n",
    "                            input_shape=(1, img_rows, img_cols)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution2D(312, 3, 3, border_mode='same', init='he_normal', W_regularizer=WeightRegularizer(l1=reg)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution2D(172, 3, 3, border_mode='same', init='he_normal', W_regularizer=WeightRegularizer(l1=reg)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution2D(172, 3, 3, border_mode='same', init='he_normal', W_regularizer=WeightRegularizer(l1=reg)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(48))\n",
    "    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    sgd = SGD(lr=learning_rate, decay=decay, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 50\n",
    "dropout = .60\n",
    "reg = 1e-5\n",
    "learning_rate = 1e-4\n",
    "decay = 1e-5\n",
    "momentum = .95\n",
    "prop_weight = .1\n",
    "\n",
    "sample_weight = np.ones((x_train.shape[0],))\n",
    "sample_weight[100:] /= prop_weight\n",
    "\n",
    "model = create_model(28, 28, learning_rate=learning_rate, reg=reg, decay = decay, dropout = dropout)\n",
    "\n",
    "model.fit(x_train, np.vstack((ytr, yprop)), batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "      shuffle=True, verbose=2, validation_data=(dat.validation.images, dat.validation.labels),\n",
    "      callbacks=[EarlyStopping(monitor='val_loss', patience=4, verbose=0)],\n",
    "            sample_weight=sample_weight)\n",
    "\n",
    "x_val = dat.validation.images\n",
    "x_val = x_val.reshape(x_val.shape[0], 1, 28, 28)\n",
    "predictions_valid = model.predict(x_val, verbose=1)\n",
    "\n",
    "print('Validation set accuracy: {}'.format((dat.validation.labels.argmax(axis=1)==predictions_valid.argmax(axis=1)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, a neural net cannot perform anywhere near this well without the propagated labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model(28, 28, learning_rate=learning_rate, reg=reg, decay = decay, dropout = dropout)\n",
    "\n",
    "model.fit(Xtr.reshape(100,1,28,28), ytr, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "      shuffle=True, verbose=2, validation_data=(dat.validation.images, dat.validation.labels),\n",
    "      callbacks=[EarlyStopping(monitor='val_loss', patience=4, verbose=0)])\n",
    "\n",
    "x_val = x_val.reshape(x_val.shape[0], 1, 28, 28)\n",
    "predictions_valid = model.predict(x_val, verbose=1)\n",
    "\n",
    "print('Validation set accuracy: {}'.format((dat.validation.labels.argmax(axis=1)==predictions_valid.argmax(axis=1)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main issues to remark on: \n",
    "1. Design choices for label propagation class\n",
    "2. Other approaches that were tried\n",
    "3. Other approaches that should be tried\n",
    "\n",
    "## Design choices for label propagation\n",
    "The main challenges in designing the label propagation class arose from limited computing time. The MNIST dataset contains 5x more examples than the original dataset of digits used by Zhu and Gharamani. So using a dense matrix to represent similarities between 55,000 datapoints, and storing these in memory is barely feasible, and performing further computations such as matrix multiplications on this is infeasibly slow. Instead, it was necessary to augment Zhu and Gharamani's algorithm to use a sparse representation for similarity. A natural choice is to compute a kernel that lists the k nearest neighbours, and this is what was done. The K nearest neighbours could be computed using a k-d tree without computing all of the distances, giving a lookup speed of d x n x log(n) where 'd' is dimensionality and 'n' is the number of datapoints rather than d x n x n for the brute force approach, but this did not give much significant speedup in practise and was in fact slower when 'n' was small. Also for reasons of limited computing time, the label propagation algorithm was parameterised with a number of iterations rather than running to convergence.\n",
    "\n",
    "A second design choice related to ensuring balance among the propagated classes. A difficulty with this algorithm is that if one class is propagated more widely than others, then in order to match their neighbours, many unlabelled nodes will also join that class. In order to avoid these bandwagon effects, \"class mass normalisation\" was performed so that the sum of the columns was made to equal zero, so that roughly equal amounts of each class were predicted. The accuracy is very inconsistent if class mass normalisation is not used.\n",
    "\n",
    "## Other approaches that were tried\n",
    "The main other approach that was tried was a self-training approach. Using built-in logistic regression and support vector machines, an effort was made to apply labels to all unlabelled examples in decreasing order of certainty. In the case of logistic regression, this was only able to improve performance by a few percentage points, to around 76%.\n",
    "\n",
    "## Other approaches that appear worth trying\n",
    "One approach that has served as a benchmark for this sort of problem, and therefore seems worth trying, is the transductive SVM. However, in recent times, the state of the art in unsupervised image classification algorithms seem to use deep learning [4], [5]. In particular, ladder networks have achieved an impressive error rate of only 1.06% using only 100 labelled examples. This level of performance seems to represent the current state of the art."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Zhu, Xiaojin. \"Semi-supervised learning.\" Encyclopedia of machine learning. Springer US, 2011. 892-897.](http://pages.cs.wisc.edu/~jerryzhu/ssl/pub/SSL_EoML.pdf)\n",
    "2. [Prakash, V. Jothi, and Dr LM Nithya. \"A survey on semi-supervised learning techniques.\" arXiv preprint arXiv:1402.4645 (2014).](http://arxiv.org/pdf/1402.4645.pdf)\n",
    "3. [Zhu, Xiaojin, and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon University, 2002.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.3864&rep=rep1&type=pdf)\n",
    "4. [Rasmus, Antti, et al. \"Semi-supervised learning with ladder networks.\" Advances in Neural Information Processing Systems. 2015.](http://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks.pdf)\n",
    "5.  [Weston, Jason, et al. \"Deep learning via semi-supervised embedding.\" Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012. 639-655.](https://infoscience.epfl.ch/record/192705/files/Weston_SPRINGER_2012.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
